# 03_微服务治理之过载保护
在学习过载保护之前，需要引入两个限流算法：
- 令牌桶算法
- 漏桶算法
## 令牌桶算法
<!-- TODO  待插入图片 -->
令牌桶算法(token-bucket rate limit algorithm)中的桶是一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌。

**算法描述如下：**
- 假设限制2r/s，则按照500毫秒的固定速率往桶中添加令牌。
- 桶中最多存放 b 个令牌，当桶满时，新添加的令牌被丢弃或拒绝。
- 当一个 n 个字节大小的数据包到达，将从桶中删除n 个令牌，接着数据包被发送到网络上。
- 如果桶中的令牌不足 n 个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。

根据算法逻辑，我们可以看到，令牌桶算法的**流入**速率是固定，但是请求的速度(或者说流出速率)是可以爆发的、无限制的。

所以一般令牌桶的算法都支持配置 `burst`(译:爆发)大小的配置，即允许请求爆发，但是有个限制。

因为我们的业务有时候会有流量突增的情况，对于令牌桶算法， 如果桶中只有100个令牌，那么其最大处理请求数也就是100了。

假设这100个令牌被突增流量取完了，而流入的速率是固定的，比如10r/s（每秒10个），那么服务处理流量的能力就会陡降到10 qps。

官方令牌桶限流算法：[`/x/time/rate`]()
## 漏桶算法
漏桶算法( leaky-bucket rate limit algorithm)作为计量工具(The Leaky Bucket Algorithm as a Meter)时，可以用于流量整形(Traffic Shaping)和流量控制(TrafficPolicing)。

<!-- TODO  待插入图片 -->

**算法的描述如下：**

- 一个固定容量的漏桶，按照常量固定速率流出水滴。
- 如果桶是空的，则不需流出水滴。
- 可以以任意速率流入水滴到漏桶。
- 如果流入水滴超出了桶的容量，则流入的水滴溢出了（被丢弃），而漏桶容量是不变的。

和令牌桶算法不同的是，漏桶算法流出速率是固定的，流入速率是任意的。

官方令牌桶限流算法：[`go.uber.org/ratelimit`]()

以上两种算法，非常适合单机版的微服务限流，思想即简单，又容易实现，有现成的库可使用，而且也是非常稳定的。

因为我们可以在某个微服务上配置一个服务总体出口的QPS值是比较方便的，然后每来一个请求，通过 `middleware` 就减1，还是较容易实现的。

漏斗桶/令牌桶确实能够保护系统不被拖垮, 但不管漏斗桶还是令牌桶, 其防护思路**都是设定一个指标**, 当超过该指标后就阻止或减少流量的继续进入，当系统负载降低到某一水平后则恢复流量的进入。

## 为什么要使用过载保护
### 案例分享--B站案例 
15、16年那时候，每次出事故都是将流量先全部干掉，然后慢慢的一点一点放开流量，让系统逐渐恢复。

- 将流量先全部干掉的原因：先让系统消化掉之前堆积的大量请求
- 缓慢注入流量：等堆积流量消化的差不多了，缓慢放行流量，优先保证系统能够正常提供服务

这样做因为如果一瞬间放入大量流量，服务本身就已经堆积了很对流量了，很可能造成系统奔溃，无法恢复流量。
### 极限压测
但是对于目前的互联网环境，上述操作并不合适，全部干掉流量相当于短时间内不提供服务，对于用户来讲，使用体验并不好。

我们可以通过**极限压测**的方式来尽量避免上述情况。

一般的（大多数）压测大都是采取下述思路:
- 流量到了一定水位线
- 故障率达到某个值
便会自动停止压测，这样做的原因是因为基于线上环境压测的，怕压测过程中出现事故，服务不可用了。

所以我们在一个高并发新服务上线前，要尽量多测试、多压测，线下压测(测试环境)时要往死里压测，因为真正出事故,服务不可用时,用户实际的使用情况是：
- APP没反应，会不断上滑或下拉来刷新，每一次刷新就意味着一次请求
- 浏览器/客户端也类似，只要不正常显示，都会疯狂的重新刷新页面

那这两类操作都会让原先的请求浏览暴增,甚至翻倍很多，很少有用户因为一次响应慢就直接退出的，绝大多数用户都会不断地重试.

比如有一个服务能够提供的最大QPS是1000,当采用极限压测策略时,如果不采取过载保护的话,那么该服务很大概率会崩溃,CPU已经100%,满负荷运载了. 再加压,那服务不就挂掉了嘛.

那如果对该服务进行了过载保护,在极限压测时,服务并不会直接崩溃,而是尽可能的提供服务,比如原来时1000的QPS,使用了过载保护后,变为900 QPS了,甚至800 QPS了. 可以看到,服务还是正常运行,并没有崩溃,只不过处理请求的最大数量减少了些.
### 阈值/指标配置不容易,耗时耗力
现在很多应用都微服务化了,可以试想一下,如果线上一个服务出事故了,可能是后端依赖的redis\MySQL\其他接口出现了问题,如果想从全局层面 

比如说:集群一共能提供10000流量,如果想控制流量暴增的情况,这时阈值/指标是不容易配置的,有以下原因:
- 如果集群有10个节点,每个节点分摊流量. 那需要人为的做除法,分摊流量到每个节点,并且要通过配置文件动态下发到每个节点.
  - 这意味着集群增加机器或者减少机器限流阈值是否要重新设置? 因为还要再做除法等,再重新配置阈值
- 设置限流阈值的依据是什么?  
  - 难道每次微服务迭代上线都要进行压测,然后得到一个相对来说比较可靠的阈值?
- 人力运维成本是否过高?
- 当调用方反馈429时, 这个时候重新设置限流, 其实流量高峰已经过了重新评估限流是否有意义?
  - 也就是说当服务提供方通知调用方配置一个更合理的阈值, 调用方再重新把新配置下发到每个服务节点,这是需要一点时间,很可能流量高峰已经过去了,那么再设置新的阈值来限流意义就不大了.  
  - 最让人难受的场景就是,工程师正常休息\休假,结果服务出问题了, 从接到电话报警,到打开电脑,登陆VPN,打开服务治理面板,再修改配置,再下发到对于的集群等等 这一连串操作不是1秒就能完成的. 当全部完成了,流量高峰也必定过去了

令牌桶\漏桶算法在上述场景中通常都是被动的，其**实际效果取决于限流阈值设置是否合理**，但往往设置合理不是一件容易的事情。

这些其实都是采用漏斗桶/令牌桶的缺点, 总体来说就是太被动, 不能快速适应流量变化。

对于要明确设定指标的思路都是相对不科学的，就像我们压测一样，针对某个服务，给出一个具体、准确的QPS是不太容易的。因为业务一直在迭代，物理机器的配置也会变化，是**挺难配置指标**的。

因此我们需要一种**自适应的限流算法**，即: **过载保护，根据系统当前的负载自动丢弃流量**。
## 过载保护--自适应的限流
过载保护,根据系统当前的负载**自动**丢弃流量,是主动丢弃的. 通过计算系统临近过载时的峰值吞吐作为限流的阈值来进行流量控制，达到系统保护。

核心思想如下:

- 服务器临近过载时，主动抛弃一定量的负载，目标是自保。
- 在系统稳定的前提下，保持系统的吞吐量。常见做法：利特尔法则 
- CPU、内存作为**信号量**进行节流。
- 队列管理: 队列长度、LIFO。
- 可控延迟算法: CoDel。


<!-- TODO  待插入图片-利特尔法则 -->
### 利特尔法则公式
公式: `L = r*w`

- l: 吞吐量
- r: 请求流入速率,QPS
- w: 请求耗时,latency

### 为什么选用CPU作为信号量
因为如果使用内存作为信号量的话, 内存涨的比较快时, 一定会触发GC.

在Go中, GC启动的场景是:
- 定时启动: 2分钟GC一次
- 内存容量变化大: 当下的内存和上一次申请的内存相比, 容量涨了一定倍数

如果GC启动了,那么CPU一定会分担GC工作,言外之意就是可以使用CPU来作为系统负载的信号量.

服务器临近过载的时候, 比如CPU 90%, 此时负载一定是处于较高的点, 然后我们通过利特尔法则计算出此时的吞吐量, 我们就认为是系统最大的吞吐量了.

有了这个最大吞吐量 L, 我们就能实现, 当服务器的流量涌入后,计算新的吞吐(L')值,:
- 比当前L **大**,那么就**丢弃**这部分请求
- 比当前L **小**,那么就**放行**这部分请求

另外,这个CPU 上限百分比还是很容易配置的, 起码比令牌桶\漏桶算法的 QPS 好配.

### 可控延迟算法--CoDel
一般做限流算法,都会将请求放到一个队列中, 然后队头有个控制器,将取到的请求进行分发(dispatch)

这会有一点的问题:
> 队列过大时,当拿到最末尾的请求(堆积在队列尾部了)时,可能已经超时了. 也就是说,在队列里排队时间越久,超时的概率越大

> 对于超时的请求,我们便不再进行处理,节省资源. 

为了尽量处理每个请求,我们使用了CoDel算法, 能够计算请求控制器从队头取出的请求在队列中待的时间长短, 这个值又叫惩罚值.

根据这个惩罚值来判断该请求是处理还是丢弃, 这样就能快速消耗掉排在队列前面的请求, 防止当轮到最末尾的请求时,发现都超时了

### 如何计算接近峰值时的系统吞吐?
<!-- TODO  待插入图片  如何计算接近峰值时的系统吞吐-->

简单来说,就是计算CPU的使用率 
- CPU: 使用一个独立的线程(在go中是goroutine)采样，每隔 250ms 触发一次。在计算均值时，使用了简单滑动平均去除峰值的影响。
- Inflight: 当前服务中正在进行的请求的数量。
- Pass&RT: 最近5s，pass 为每100ms采样窗口内成功请求的数量，rt 为单个采样窗口中平均响应时间

#### CPU 采样
每隔一段时间对CPU进行采样, 然后计算CPU使用率: (B-N)/T
- B(Before): 上一次采用值. 
- N(Now): 当前采用值
- T(Time): 间隔时间

具体的样本值B或N(tiket?)的意义是: CPU用来干活的时间值, 它是自增的(counter), 除了重启

`(B-N)/T` 这个公式其实就是计算平均值. 在实际应用时,发现并不准确, 反映到图线上就是抖动很明显, CPU使用率变化较大.

为了减少这种抖动, 采用了滑动均值(Moving average)来去除峰值的影响. 

滑动均值算法公式: vt =β * vt + (1-β)*∮
- vt: 当前平均值, 也就是(B-N)/T计算的平均值
- β: 一个给定的值
- ∮: 上一个平均值

示例:  
- vt: 90%
- β:  0.7
- ∮: 80%
  
滑动均值: vt = 0.7 * 90% + 0.3 * 80%

使用了滑动均值后, 带来了很大好处: 
- 当CPU是抖动,可以去噪, 去掉毛刺
- 当CPU爆发增长幅度很大, CPU的响应也会很快. 因为 β是针对当前平均值而定的系统,是一个大于0.5的值,如0.7, 那么过去的值占比就较小了(1-β)=0.3
